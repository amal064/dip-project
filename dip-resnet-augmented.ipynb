{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":840806,"sourceType":"datasetVersion","datasetId":59760},{"sourceId":2227838,"sourceType":"datasetVersion","datasetId":1335579},{"sourceId":2409323,"sourceType":"datasetVersion","datasetId":1457432}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.datasets import ImageFolder\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm import tqdm\nfrom torch.utils.data import random_split\nfrom PIL import Image \ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T07:53:50.095749Z","iopub.execute_input":"2025-04-24T07:53:50.096143Z","iopub.status.idle":"2025-04-24T07:53:58.976580Z","shell.execute_reply.started":"2025-04-24T07:53:50.096125Z","shell.execute_reply":"2025-04-24T07:53:58.975950Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\nimport matplotlib.pyplot as plt\nimport cv2\nimport numpy as np\nfrom typing import Tuple\nimport torch.fft\nfrom torch import Tensor\n\nimport torch\nfrom torch import Tensor\nfrom torch.distributions import Uniform\nfrom torch.distributions.bernoulli import Bernoulli\n\nclass GaussianMixtureMask:\n    \"\"\"Applies a Gaussian Mixture Mask in the Fourier domain to an image.\n\n    The mask is created using random Gaussian kernels, which are applied in\n    the frequency domain.\n\n    Attributes:\n        num_gaussians: Number of Gaussian kernels to generate in the mixture mask.\n        std_range: Tuple containing the minimum and maximum standard deviation for the Gaussians.\n    \"\"\"\n\n    def __init__(\n        self, num_gaussians: int = 20, std_range: Tuple[float, float] = (10, 15)\n    ):\n        \"\"\"Initializes GaussianMixtureMasks with the given parameters.\n\n        Args:\n            num_gaussians: Number of Gaussian kernels to generate in the mixture mask.\n            std_range: Tuple containing the minimum and maximum standard deviation for the Gaussians.\n        \"\"\"\n        self.num_gaussians = num_gaussians\n        self.std_range = std_range\n\n    def gaussian_kernel(\n        self, size: Tuple[int, int], sigma: Tensor, center: Tensor\n    ) -> Tensor:\n        \"\"\"Generates a 2D Gaussian kernel.\n\n        Args:\n            size: Tuple specifying the dimensions of the Gaussian kernel (H, W).\n            sigma: Tensor specifying the standard deviation of the Gaussian.\n            center: Tensor specifying the center of the Gaussian kernel.\n\n        Returns:\n            A 2D Gaussian kernel tensor.\n        \"\"\"\n        u, v = torch.meshgrid(torch.arange(0, size[0]), torch.arange(0, size[1]))\n        u = u.to(sigma.device)\n        v = v.to(sigma.device)\n        u0, v0 = center\n        gaussian = torch.exp(\n            -((u - u0) ** 2 / (2 * sigma[0] ** 2) + (v - v0) ** 2 / (2 * sigma[1] ** 2))\n        )\n\n        return gaussian\n\n    def apply_gaussian_mixture_mask(\n        self, freq_image: Tensor, num_gaussians: int, std: Tuple[float, float]\n    ) -> Tensor:\n        \"\"\"Applies the Gaussian mixture mask to a frequency-domain image.\n\n        Args:\n            freq_image: Tensor representing the frequency-domain image of shape (C, H, W//2+1).\n            num_gaussians: Number of Gaussian kernels to generate in the mask.\n            std: Tuple specifying the standard deviation range for the Gaussians.\n\n        Returns:\n            Image tensor in frequency domain after applying the Gaussian mixture mask.\n        \"\"\"\n        (C, U, V) = freq_image.shape\n        mask = freq_image.new_ones(freq_image.shape)\n\n        for _ in range(num_gaussians):\n            u0 = torch.randint(0, U, (1,), device=freq_image.device)\n            v0 = torch.randint(0, V, (1,), device=freq_image.device)\n            center = torch.tensor((u0, v0), device=freq_image.device)\n            sigma = torch.rand(2, device=freq_image.device) * (std[1] - std[0]) + std[0]\n\n            g_kernel = self.gaussian_kernel((U, V), sigma, center)\n            mask *= 1 - g_kernel.unsqueeze(0)\n\n        filtered_freq_image = freq_image * mask\n        return filtered_freq_image\n\n    def __call__(self, freq_image: Tensor) -> Tensor:\n        \"\"\"Applies the Gaussian mixture mask transformation to the input frequency-domain image.\n\n        Args:\n            freq_image: Tensor representing a frequency-domain image of shape (C, H, W//2+1).\n\n        Returns:\n            Image tensor in frequency domain after applying the Gaussian mixture mask.\n        \"\"\"\n        return self.apply_gaussian_mixture_mask(\n            freq_image, self.num_gaussians, self.std_range\n        )\n\n\n\nclass IRFFT2DTransform:\n    \"\"\"Inverse 2D Fast Fourier Transform (IRFFT2D) Transformation.\n\n    This transformation applies the inverse 2D Fast Fourier Transform (IRFFT2D)\n    to an image in the frequency domain.\n\n    Input:\n        - Tensor of shape (C, H, W), where C is the number of channels.\n\n    Output:\n        - Tensor of shape (C, H, W), where C is the number of channels.\n    \"\"\"\n\n    def __init__(self, shape: Tuple[int, int]):\n        \"\"\"\n        Args:\n            shape: The desired output shape (H, W) after applying the inverse FFT\n        \"\"\"\n        self.shape = shape\n\n    def __call__(self, freq_image: Tensor) -> Tensor:\n        \"\"\"Applies the inverse 2D Fast Fourier Transform (IRFFT2D) to the input tensor.\n\n        Args:\n            freq_image: A tensor in the frequency domain of shape (C, H, W).\n\n        Returns:\n            Tensor: Reconstructed image after applying IRFFT2D, of shape (C, H, W).\n        \"\"\"\n        reconstructed_image: Tensor = torch.fft.irfft2(freq_image, s=self.shape)\n        return reconstructed_image\n\n\n\nclass RFFT2DTransform:\n    \"\"\"2D Fast Fourier Transform (RFFT2D) Transformation.\n\n    This transformation applies the 2D Fast Fourier Transform (RFFT2D)\n    to an image, converting it from the spatial domain to the frequency domain.\n\n    Input:\n        - Tensor of shape (C, H, W), where C is the number of channels.\n\n    Output:\n        - Tensor of shape (C, H, W) in the frequency domain, where C is the number of channels.\n    \"\"\"\n\n    def __call__(self, image: Tensor) -> Tensor:\n        \"\"\"Applies the 2D Fast Fourier Transform (RFFT2D) to the input image.\n\n        Args:\n            image: Input image as a Tensor of shape (C, H, W).\n\n        Returns:\n            Tensor: The image in the frequency domain after applying RFFT2D, of shape (C, H, W).\n        \"\"\"\n\n        rfft_image: Tensor = torch.fft.rfft2(image)\n        return rfft_image\n\nfrom typing import Tuple\n\nclass AmplitudeRescaleTransform:\n    \"\"\"Implementation of amplitude rescaling transformation.\n\n    This transform will rescale the amplitude of the Fourier Spectrum (`freq_image`) of the image and return it.\n\n    Attributes:\n        dist:\n            Uniform distribution in `[m, n)` from which the scaling value will be selected.\n        \"\"\"\n\n    def __init__(self, range: Tuple[float, float] = (0.8, 1.75)) -> None:\n        self.dist = Uniform(range[0], range[1])\n\n    def __call__(self, freq_image: Tensor) -> Tensor:\n        amplitude = torch.sqrt(freq_image.real**2 + freq_image.imag**2)\n\n        phase = torch.atan2(freq_image.imag, freq_image.real)\n        # p with shape (H, W)\n        p = self.dist.sample(freq_image.shape[1:]).to(freq_image.device)\n        # Unsqueeze to add channel dimension.[]\n        amplitude *= p.unsqueeze(0)\n        real = amplitude * torch.cos(phase)\n        imag = amplitude * torch.sin(phase)\n        output = torch.complex(real, imag)\n\n        return output\n    \n\nclass RandomFrequencyMaskTransform:\n    \"\"\"2D Random Frequency Mask Transformation.\n\n    This transformation applies a binary mask on the fourier transform,\n    across all channels. A proportion of k frequencies are set to 0 with this.\n\n    Input\n        - Tensor: RFFT of a 2D Image (C, H, W) C-> No. of Channels\n    Output\n        - Tensor: The masked RFFT of the image\n\n    \"\"\"\n\n    def __init__(self, k: Tuple[float, float] = (0.01, 0.1)) -> None:\n        self.k = k\n\n    def __call__(self, fft_image: Tensor) -> Tensor:\n        k = np.random.uniform(low=self.k[0], high=self.k[1])\n\n        # Every mask for every channel will have same frequencies being turned off i.e. being set to zero\n        mask = (\n            torch.rand(fft_image.shape[1:], device=fft_image.device) > k\n        )  # mask_type: (H, W)\n\n        # Do not mask zero frequency mode to retain majority of the semantic information.\n        # Please refer https://arxiv.org/abs/2312.02205\n        mask[0, 0] = 1\n\n        # Adding channel dimension\n        mask = mask.unsqueeze(0)\n\n        masked_frequency_spectrum_image = fft_image * mask\n\n        return masked_frequency_spectrum_image\n    \n\nclass PhaseShiftTransform:\n    \"\"\"Implementation of phase shifting transformation.\n\n\n    Applies a random phase shift `theta` (positive or negative) to the Fourier spectrum (`freq_image`) of the image and returns the transformed spectrum.\n\n    Attributes:\n        dist:\n            A uniform distribution in the range `[p, q)` from which the magnitude of the\n            phase shift `theta` is selected.\n        include_negatives:\n            A flag indicating whether negative values of `theta` should be included.\n            If `True`, both positive and negative shifts are applied.\n        sign_dist:\n            A Bernoulli distribution used to decide the sign of `theta`, based on a\n            given probability `sign_probability`, if negative values are included.\n    \"\"\"\n\n    def __init__(\n        self,\n        range: Tuple[float, float] = (0.4, 0.7),\n        include_negatives: bool = False,\n        sign_probability: float = 0.5,\n    ) -> None:\n        self.dist = Uniform(range[0], range[1])\n        self.include_negatives = include_negatives\n        if include_negatives:\n            self.sign_dist = Bernoulli(sign_probability)\n\n    def __call__(self, freq_image: Tensor) -> Tensor:\n        # Calculate amplitude and phase\n        amplitude = torch.sqrt(freq_image.real**2 + freq_image.imag**2)\n        phase = torch.atan2(freq_image.imag, freq_image.real)\n\n        # Sample a random phase shift θ\n        theta = self.dist.sample().to(freq_image.device)\n\n        if self.include_negatives:\n            # Determine sign for shift: +θ or -θ\n            sign = self.sign_dist.sample().to(freq_image.device)\n            # Apply random sign directly to theta\n            theta = torch.where(sign == 1, theta, -theta)\n\n        # Adjust the phase\n        phase_shifted = phase + theta\n\n        # Recreate the complex spectrum with adjusted phase\n        real = amplitude * torch.cos(phase_shifted)\n        imag = amplitude * torch.sin(phase_shifted)\n        output = torch.complex(real, imag)\n\n        return output\n\ndef GaussianMixture(image,num_gaussians=15):\n    gaussian_mixture = GaussianMixtureMask(num_gaussians=num_gaussians, std_range=(10, 15))\n    transform=RFFT2DTransform()\n    \n    image_tensor = torch.unsqueeze(torch.tensor(image, dtype=torch.float32, device=device), dim=0)\n    freq_image=transform(image_tensor)\n    transformed_image_tensor = gaussian_mixture(freq_image)\n    \n    image_size = freq_image.shape[1:]\n    original_height = image_size[0]\n    original_width = 2 * (image_size[1] - 1)\n\n    original_shape = (original_height, original_width)\n\n    irfft2d_transform = IRFFT2DTransform(original_shape)\n    \n    transformed_image_tensor = irfft2d_transform(transformed_image_tensor)\n    \n    transformed_image = transformed_image_tensor.squeeze()\n    return transformed_image\n\n\ndef AmplitudeRescale(image):\n    apl_rescale = AmplitudeRescaleTransform(range=(0.8, 1.75))\n    transform=RFFT2DTransform()\n    \n    image_tensor = torch.unsqueeze(torch.tensor(image, dtype=torch.float32, device=device), dim=0)\n    freq_image=transform(image_tensor)\n    transformed_image_tensor = apl_rescale(freq_image)\n    \n    image_size = freq_image.shape[1:]\n    original_height = image_size[0]\n    original_width = 2 * (image_size[1] - 1)\n\n    original_shape = (original_height, original_width)\n\n    irfft2d_transform = IRFFT2DTransform(original_shape)\n    \n    transformed_image_tensor = irfft2d_transform(transformed_image_tensor)\n    \n    transformed_image = transformed_image_tensor.squeeze()\n    return transformed_image\n\n\ndef RandomFrequencyMask(image):\n    rand_freq = RandomFrequencyMaskTransform(k=(0.01, 0.1))\n    transform=RFFT2DTransform()\n    \n    image_tensor = torch.unsqueeze(torch.tensor(image, dtype=torch.float32, device=device), dim=0)\n    freq_image=transform(image_tensor)\n    transformed_image_tensor = rand_freq(freq_image)\n    \n    image_size = freq_image.shape[1:]\n    original_height = image_size[0]\n    original_width = 2 * (image_size[1] - 1)\n\n    original_shape = (original_height, original_width)\n\n    irfft2d_transform = IRFFT2DTransform(original_shape)\n    \n    transformed_image_tensor = irfft2d_transform(transformed_image_tensor)\n    \n    transformed_image = transformed_image_tensor.squeeze()\n    return transformed_image\n\n\ndef PhaseShift(image):\n    phase_shift = PhaseShiftTransform(range=(0.4, 0.7), include_negatives=True, sign_probability=0.5)\n    transform=RFFT2DTransform()\n    \n    image_tensor = torch.unsqueeze(torch.tensor(image, dtype=torch.float32, device=device), dim=0)\n    freq_image=transform(image_tensor)\n    transformed_image_tensor = phase_shift(freq_image)\n    \n    image_size = freq_image.shape[1:]\n    original_height = image_size[0]\n    original_width = 2 * (image_size[1] - 1)\n\n    original_shape = (original_height, original_width)\n\n    irfft2d_transform = IRFFT2DTransform(original_shape)\n    \n    transformed_image_tensor = irfft2d_transform(transformed_image_tensor)\n    \n    transformed_image = transformed_image_tensor.squeeze()\n    return transformed_image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T07:53:58.977915Z","iopub.execute_input":"2025-04-24T07:53:58.978331Z","iopub.status.idle":"2025-04-24T07:53:59.373773Z","shell.execute_reply.started":"2025-04-24T07:53:58.978279Z","shell.execute_reply":"2025-04-24T07:53:59.373086Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import os\nfrom PIL import Image\nfrom torch.utils.data import Dataset\n\nclass AnimalDataset(Dataset):\n    def __init__(self, root_dir, transform=None,augmentation_fn=None):\n        \"\"\"\n        Args:\n          root_dir (str): Directory with subfolders per class.\n          transform (callable, optional): Transform to apply to PIL images.\n        \"\"\"\n        self.root_dir = root_dir\n        self.transform = transform\n\n        # Build list of (image_path, label) pairs\n        self.samples = []\n        self.class_to_idx = {}\n        for idx, class_name in enumerate(sorted(os.listdir(root_dir))):\n            class_folder = os.path.join(root_dir, class_name)\n            if not os.path.isdir(class_folder):\n                continue\n            self.class_to_idx[class_name] = idx\n            for fname in os.listdir(class_folder):\n                if fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n                    path = os.path.join(class_folder, fname)\n                    self.samples.append((path, idx))\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, index):\n        path, label = self.samples[index]\n        image = Image.open(path).convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n        return image, label\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-24T07:53:59.374500Z","iopub.execute_input":"2025-04-24T07:53:59.374739Z","iopub.status.idle":"2025-04-24T07:53:59.381056Z","shell.execute_reply.started":"2025-04-24T07:53:59.374718Z","shell.execute_reply":"2025-04-24T07:53:59.380337Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import torch\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader, random_split\n\nimport random\n\ndef my_fourier_augmentation(image):\n    transforms = [\n        GaussianMixture,\n        AmplitudeRescale,\n        RandomFrequencyMask,\n        PhaseShift\n    ]\n    lmage = random.choice(transforms)(image)\n    return image\n\n# 1) Transforms: resize, to tensor, normalize (ImageNet stats)\ndata_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485,0.456,0.406],\n                         std =[0.229,0.224,0.225])\n])\n\n# 2) Instantiate full dataset\nfull_dataset = AnimalDataset(root_dir='/kaggle/input/animals10/raw-img', transform=data_transform, augmentation_fn=my_fourier_augmentation)\n\n\n# 3) Split into train (80%) and test (20%)\ntrain_size = int(0.8 * len(full_dataset))\ntest_size  = len(full_dataset) - train_size\ntrain_ds, test_ds = random_split(full_dataset, [train_size, test_size])\n# Since PyTorch v1.13, you can also pass floats: [0.8, 0.2] :contentReference[oaicite:6]{index=6}\n\n# 4) DataLoaders for batching\ntrain_loader = DataLoader(train_ds, batch_size=32, shuffle=True,  num_workers=4)\ntest_loader  = DataLoader(test_ds,  batch_size=32, shuffle=False, num_workers=4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T07:53:59.382723Z","iopub.execute_input":"2025-04-24T07:53:59.382907Z","iopub.status.idle":"2025-04-24T07:54:01.140537Z","shell.execute_reply.started":"2025-04-24T07:53:59.382892Z","shell.execute_reply":"2025-04-24T07:54:01.139682Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.optim as optim\nimport torchvision.models as models\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load pretrained ResNet-18 and replace final FC layer\nmodel = models.resnet18(pretrained=True)\nnum_classes = len(full_dataset.class_to_idx)\nmodel.fc = nn.Linear(model.fc.in_features, num_classes)\nmodel = model.to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.fc.parameters(), lr=1e-3)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T07:54:01.141450Z","iopub.execute_input":"2025-04-24T07:54:01.141755Z","iopub.status.idle":"2025-04-24T07:54:02.091280Z","shell.execute_reply.started":"2025-04-24T07:54:01.141714Z","shell.execute_reply":"2025-04-24T07:54:02.090514Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00<00:00, 179MB/s] \n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from tqdm import tqdm\n\nnum_epochs = 10\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss, running_corrects = 0.0, 0\n\n    # Wrap the training loader with tqdm\n    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False):\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() * inputs.size(0)\n        preds = outputs.argmax(dim=1)\n        running_corrects += (preds == labels).sum().item()\n\n    epoch_loss = running_loss / train_size\n    epoch_acc = running_corrects / train_size * 100\n    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.2f}%\")\n\n    # Evaluation\n    model.eval()\n    test_corrects, test_total = 0, 0\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            preds = outputs.argmax(dim=1)\n            test_corrects += (preds == labels).sum().item()\n            test_total += labels.size(0)\n\n    test_acc = test_corrects / test_total * 100\n    print(f\"          Test Acc: {test_acc:.2f}%\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T07:54:17.190967Z","iopub.execute_input":"2025-04-24T07:54:17.191449Z","iopub.status.idle":"2025-04-24T08:07:16.760848Z","shell.execute_reply.started":"2025-04-24T07:54:17.191424Z","shell.execute_reply":"2025-04-24T08:07:16.759938Z"}},"outputs":[{"name":"stderr","text":"                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10 - Train Loss: 0.3666, Train Acc: 90.15%\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"          Test Acc: 94.65%\n\n","output_type":"stream"},{"name":"stderr","text":"                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 2/10 - Train Loss: 0.1912, Train Acc: 94.09%\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"          Test Acc: 94.71%\n\n","output_type":"stream"},{"name":"stderr","text":"                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 3/10 - Train Loss: 0.1695, Train Acc: 94.69%\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"          Test Acc: 95.30%\n\n","output_type":"stream"},{"name":"stderr","text":"                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 4/10 - Train Loss: 0.1536, Train Acc: 95.29%\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"          Test Acc: 94.92%\n\n","output_type":"stream"},{"name":"stderr","text":"                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 5/10 - Train Loss: 0.1495, Train Acc: 95.28%\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"          Test Acc: 94.84%\n\n","output_type":"stream"},{"name":"stderr","text":"                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 6/10 - Train Loss: 0.1456, Train Acc: 95.23%\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"          Test Acc: 95.47%\n\n","output_type":"stream"},{"name":"stderr","text":"                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 7/10 - Train Loss: 0.1404, Train Acc: 95.57%\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"          Test Acc: 94.86%\n\n","output_type":"stream"},{"name":"stderr","text":"                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 8/10 - Train Loss: 0.1394, Train Acc: 95.65%\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"          Test Acc: 95.02%\n\n","output_type":"stream"},{"name":"stderr","text":"                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 9/10 - Train Loss: 0.1313, Train Acc: 95.69%\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"          Test Acc: 95.19%\n\n","output_type":"stream"},{"name":"stderr","text":"                                                              ","output_type":"stream"},{"name":"stdout","text":"Epoch 10/10 - Train Loss: 0.1313, Train Acc: 95.84%\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"          Test Acc: 95.53%\n\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}